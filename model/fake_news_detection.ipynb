{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: numpy in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (1.19.2)\n",
      "Requirement already satisfied: pandas in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: six in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: sklearn in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: bs4 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /Users/silicon/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install Packages\n",
    "# !pip install gdown\n",
    "\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install sklearn\n",
    "\n",
    "!pip install bs4"
   ]
  },
  {
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import gdown\n",
    "from zipfile import ZipFile\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'gdown' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2665500f470c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/uc?id=1O5-lPev_Z0XysVdtlC-RDjlanyCjDy3x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mzipObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gdown' is not defined"
     ]
    }
   ],
   "source": [
    "# Download Data\n",
    "gdown.download('https://drive.google.com/uc?id=1O5-lPev_Z0XysVdtlC-RDjlanyCjDy3x', 'data.zip', quiet=False)\n",
    "with ZipFile('data.zip', 'r') as zipObj:\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data and Extract Columns\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data = train_data.fillna(' ')\n",
    "train_data['combined'] = train_data['title'] + ' ' + train_data['author'] + ' '  + train_data['text']\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_labels = pd.read_csv('submit.csv')\n",
    "test_data['label'] = test_labels['label']\n",
    "test_data = test_data.fillna(' ')\n",
    "test_data['combined'] = test_data['title'] + ' ' + test_data['author'] + ' '  + test_data['text']\n",
    "\n",
    "train_text = train_data['combined'].to_numpy().astype(str)\n",
    "y_train = train_data['label'].to_numpy()\n",
    "\n",
    "test_text = test_data['combined'].to_numpy().astype(str)\n",
    "y_test = test_data['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c2653c247a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# dump(text_tf_idf_vectorizer,'text_tf_idf_vectorizer.joblib')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtext_tf_idf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_tf_idf_vectorizer.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tf_idf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorize Data for 'Article Text'\n",
    "\n",
    "# fyi, running TfidfVectorizer() can take a long time, so just \n",
    "# load the joblib file if you can\n",
    "\n",
    "# text_tf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),stop_words='english')\n",
    "# text_tf_idf_vectorizer.fit(train_text)\n",
    "# dump(text_tf_idf_vectorizer,'text_tf_idf_vectorizer.joblib')\n",
    "\n",
    "text_tf_idf_vectorizer = load('text_tf_idf_vectorizer.joblib')\n",
    "\n",
    "X_train = text_tf_idf_vectorizer.transform(train_text)\n",
    "X_test = text_tf_idf_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'RandomizedSearchCV' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f86d7cda73c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                }\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m text_clf_rs = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=seed),\n\u001b[0m\u001b[1;32m      9\u001b[0m                                 \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                 cv=3, verbose=10, random_state=seed, n_jobs = -1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomizedSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform Hyperparameter Random Search\n",
    "random_grid = {\n",
    "                'n_estimators': [100, 500, 1000, 1500, 2000],\n",
    "                'max_depth': [1, 2, 4, 8, 10, 16, 20, None],\n",
    "                'max_features': ['sqrt', 'log2'],\n",
    "               }\n",
    "\n",
    "text_clf_rs = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=seed),\n",
    "                                param_distributions=random_grid, n_iter = 20, \n",
    "                                cv=3, verbose=10, random_state=seed, n_jobs = -1)\n",
    "\n",
    "text_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(text_clf_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Evaluate Classifier on 'Article Text'\n",
    "text_clf = load('fake_news_classifier.joblib')\n",
    "\n",
    "# text_clf = text_clf_rs.best_estimator_\n",
    "# text_clf = RandomForestClassifier(n_estimators = 1000, max_depth=4, random_state=seed, n_jobs=-1, verbose=1)\n",
    "\n",
    "# text_clf.fit(X_train, y_train)\n",
    "# dump(text_clf,'fake_news_classifier.joblib')\n",
    "\n",
    "text_clf_acc = text_clf.score(X_test, y_test)\n",
    "\n",
    "print('Article Text Classifier Accuracy: ' + str(text_clf_acc))\n",
    "\n",
    "plot_confusion_matrix(text_clf, X_test, y_test, cmap='Blues', values_format = 'd', display_labels=['Fake','True'])  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:   19.9s\n",
      "(0.7984803435744962, 0.8448095071653268, 0.8209918478260869, None)\n",
      "0.7984803435744962\n",
      "0.8448095071653268\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:   25.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "y_pred = text_clf.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='binary'))\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(precision_score(y_true, y_pred))\n",
    "print(recall_score(y_true, y_pred))"
   ]
  },
  {
   "source": [
    "### Extract Text From Webpage Link"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b10b2fc8e6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpost_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' Woah, check out https://www.thegatewaypundit.com/2021/02/ignored-media-dirtbag-joe-biden-says-us-veterans-former-police-officers-fueling-white-supremacism-america/ here'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlinks_in_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(https?://\\S+)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks_in_post\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "post_text = ' Woah, check out https://www.thegatewaypundit.com/2021/02/ignored-media-dirtbag-joe-biden-says-us-veterans-former-police-officers-fueling-white-supremacism-america/ here'\n",
    "\n",
    "links_in_post = re.findall(r'(https?://\\S+)', post_text)\n",
    "\n",
    "for link in links_in_post:\n",
    "    raw_html = requests.get(link).text\n",
    "    webpage_text = BeautifulSoup(raw_html).text\n",
    "\n",
    "    webpage_vector = text_tf_idf_vectorizer.transform([webpage_text])\n",
    "\n",
    "    print(text_clf.predict(webpage_vector)[0])\n",
    "    print(text_clf.predict_proba(webpage_vector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "0\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    3.1s\n",
      "[0.50874747 0.49125253]\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    4.1s finished\n"
     ]
    }
   ],
   "source": [
    "post_text = ' Woah, check out https://www.breitbart.com/politics/2015/08/08/climate-change-the-hoax-that-costs-us-4-billion-a-day/ here'\n",
    "\n",
    "links_in_post = re.findall(r'(https?://\\S+)', post_text)\n",
    "\n",
    "for link in links_in_post:\n",
    "    raw_html = requests.get(link).text\n",
    "    webpage_text = BeautifulSoup(raw_html).text\n",
    "\n",
    "    webpage_vector = text_tf_idf_vectorizer.transform([webpage_text])\n",
    "\n",
    "    print(text_clf.predict(webpage_vector)[0])\n",
    "    print(text_clf.predict_proba(webpage_vector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "0\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    2.4s\n",
      "[0.51139215 0.48860785]\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "post_text = 'This is not very cash money: https://money.cnn.com/2018/05/10/technology/russian-facebook-ads-targeted-mexican-americans/index.html'\n",
    "\n",
    "links_in_post = re.findall(r'(https?://\\S+)', post_text)\n",
    "\n",
    "for link in links_in_post:\n",
    "    raw_html = requests.get(link).text\n",
    "    webpage_text = BeautifulSoup(raw_html).text\n",
    "\n",
    "    webpage_vector = text_tf_idf_vectorizer.transform([webpage_text])\n",
    "\n",
    "    print(text_clf.predict(webpage_vector)[0])\n",
    "    print(text_clf.predict_proba(webpage_vector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}